{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# if above not worked then run command \n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains. \n",
    "Data science is related to data mining, machine learning and big data. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Atul\n",
      "[nltk_data]     Gaikwad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n",
       " 'Data science is related to data mining, machine learning and big data.',\n",
       " 'However, data science is different from computer science and information science.',\n",
       " 'Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " ',',\n",
       " 'processes',\n",
       " ',',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'systems',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'noisy',\n",
       " ',',\n",
       " 'structured',\n",
       " 'and',\n",
       " 'unstructured',\n",
       " 'data',\n",
       " ',',\n",
       " '[',\n",
       " '1',\n",
       " ']',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " 'and',\n",
       " 'apply',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'actionable',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'data',\n",
       " 'across',\n",
       " 'a',\n",
       " 'broad',\n",
       " 'range',\n",
       " 'of',\n",
       " 'application',\n",
       " 'domains',\n",
       " '.',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'related',\n",
       " 'to',\n",
       " 'data',\n",
       " 'mining',\n",
       " ',',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'big',\n",
       " 'data',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'different',\n",
       " 'from',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'and',\n",
       " 'information',\n",
       " 'science',\n",
       " '.',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " 'winner',\n",
       " 'Jim',\n",
       " 'Gray',\n",
       " 'imagined',\n",
       " 'data',\n",
       " 'science',\n",
       " 'as',\n",
       " 'a',\n",
       " '``',\n",
       " 'fourth',\n",
       " 'paradigm',\n",
       " \"''\",\n",
       " 'of',\n",
       " 'science',\n",
       " '(',\n",
       " 'empirical',\n",
       " ',',\n",
       " 'theoretical',\n",
       " ',',\n",
       " 'computational',\n",
       " ',',\n",
       " 'and',\n",
       " 'now',\n",
       " 'data-driven',\n",
       " ')',\n",
       " 'and',\n",
       " 'asserted',\n",
       " 'that',\n",
       " '``',\n",
       " 'everything',\n",
       " 'about',\n",
       " 'science',\n",
       " 'is',\n",
       " 'changing',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'impact',\n",
       " 'of',\n",
       " 'information',\n",
       " 'technology',\n",
       " \"''\",\n",
       " 'and',\n",
       " 'the',\n",
       " 'data',\n",
       " 'deluge',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"we are trying to test the nltk library in an ineuaron class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Atul Gaikwad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('trying', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('test', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('nltk', 'JJ'),\n",
       " ('library', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('ineuaron', 'NN'),\n",
       " ('class', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parts of Speech of text for each words in it \n",
    "# PRP personal pronoun I, he, she\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# DT determiner\n",
    "# JJ adjective ‘big’\n",
    "# IN preposition/subordinating conjunction \n",
    "# many more ... find pos nltk list in online .........\n",
    "\n",
    "nltk.pos_tag(nltk.word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Atul\n",
      "[nltk_data]     Gaikwad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords\n",
    "# for other language may need to download corpus for that particaulr lang or can use libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "punctuation = string.punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'uses',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " 'processes',\n",
       " 'algorithms',\n",
       " 'systems',\n",
       " 'extract',\n",
       " 'knowledge',\n",
       " 'insights',\n",
       " 'noisy',\n",
       " 'structured',\n",
       " 'unstructured',\n",
       " 'data',\n",
       " ',[',\n",
       " '1',\n",
       " '][',\n",
       " '2',\n",
       " 'apply',\n",
       " 'knowledge',\n",
       " 'actionable',\n",
       " 'insights',\n",
       " 'data',\n",
       " 'across',\n",
       " 'broad',\n",
       " 'range',\n",
       " 'application',\n",
       " 'domains',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'related',\n",
       " 'data',\n",
       " 'mining',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'big',\n",
       " 'data',\n",
       " 'However',\n",
       " 'data',\n",
       " 'science',\n",
       " 'different',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'information',\n",
       " 'science',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " 'winner',\n",
       " 'Jim',\n",
       " 'Gray',\n",
       " 'imagined',\n",
       " 'data',\n",
       " 'science',\n",
       " 'fourth',\n",
       " 'paradigm',\n",
       " 'science',\n",
       " 'empirical',\n",
       " 'theoretical',\n",
       " 'computational',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'asserted',\n",
       " 'everything',\n",
       " 'science',\n",
       " 'changing',\n",
       " 'impact',\n",
       " 'information',\n",
       " 'technology',\n",
       " 'data',\n",
       " 'deluge']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "for word in nltk.wordpunct_tokenize(text):\n",
    "    if word not in punctuation:\n",
    "        if word not in stopwords:\n",
    "            lst.append(word)\n",
    "\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'NNP'),\n",
       " ('science', 'NN'),\n",
       " ('interdisciplinary', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('uses', 'VBZ'),\n",
       " ('scientific', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " ('processes', 'VBZ'),\n",
       " ('algorithms', 'JJ'),\n",
       " ('systems', 'NNS'),\n",
       " ('extract', 'JJ'),\n",
       " ('knowledge', 'JJ'),\n",
       " ('insights', 'NNS'),\n",
       " ('noisy', 'RB'),\n",
       " ('structured', 'VBD'),\n",
       " ('unstructured', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " (',[', '$'),\n",
       " ('1', 'CD'),\n",
       " ('][', '$'),\n",
       " ('2', 'CD'),\n",
       " ('apply', 'NN'),\n",
       " ('knowledge', 'NN'),\n",
       " ('actionable', 'JJ'),\n",
       " ('insights', 'NNS'),\n",
       " ('data', 'NNS'),\n",
       " ('across', 'IN'),\n",
       " ('broad', 'JJ'),\n",
       " ('range', 'NN'),\n",
       " ('application', 'NN'),\n",
       " ('domains', 'VBZ'),\n",
       " ('Data', 'NNP'),\n",
       " ('science', 'NN'),\n",
       " ('related', 'VBN'),\n",
       " ('data', 'NNS'),\n",
       " ('mining', 'NN'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('big', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('However', 'RB'),\n",
       " ('data', 'VBP'),\n",
       " ('science', 'NN'),\n",
       " ('different', 'JJ'),\n",
       " ('computer', 'NN'),\n",
       " ('science', 'NN'),\n",
       " ('information', 'NN'),\n",
       " ('science', 'NN'),\n",
       " ('Turing', 'NNP'),\n",
       " ('Award', 'NNP'),\n",
       " ('winner', 'NN'),\n",
       " ('Jim', 'NNP'),\n",
       " ('Gray', 'NNP'),\n",
       " ('imagined', 'VBD'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('fourth', 'JJ'),\n",
       " ('paradigm', 'NN'),\n",
       " ('science', 'NN'),\n",
       " ('empirical', 'JJ'),\n",
       " ('theoretical', 'JJ'),\n",
       " ('computational', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('driven', 'RB'),\n",
       " ('asserted', 'VBD'),\n",
       " ('everything', 'NN'),\n",
       " ('science', 'NN'),\n",
       " ('changing', 'VBG'),\n",
       " ('impact', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('technology', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('deluge', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
    "# just chop words with suffix s,er,ies,ing,etc into original word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all this stem class gives almost simmilar result, in case some difference may be there \n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hobbi'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"hobbies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hobbi'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"hobby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atul'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"atul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'got'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"got\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Atul\n",
      "[nltk_data]     Gaikwad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemitize will try to convert word into its base word \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lema = WordNetLemmatizer()\n",
    "lema.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema.lemmatize(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema.lemmatize(\"went\") # here default pos is ='n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema.lemmatize(\"went\",pos='v') # with pos = 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema.lemmatize(\"gone\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Went'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema.lemmatize(\"Went\",pos='v') # not case sensetive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF\n",
    "#Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review 1 \n",
    "review_lst = []\n",
    "review_lst.append( \"\"\"Best coaching ever i found online,\n",
    "Shudhanshu and Krish have amazing teaching skill,\n",
    "You guys are really doing Great for society as well by providing affordable coarse.\n",
    "Thanks a Lot.\n",
    "\"\"\")\n",
    "# Positive \n",
    "\n",
    "# review 2\n",
    "review_lst.append(\"\"\"\n",
    "I have enrolled in the Machine Learning and Deep learning course. \n",
    "The course content was very good in such a way that even a person from Non-IT background can understand very well. Krish and Sudanshu, You people are really awesome and  I was lucky to have the teachers like you.The support team is quick and highly responsive.The course fee is highly affordable.You are making the career transitions smoother for us.I hope i will be able to do the career transition with your guidance in next 3-4 months\n",
    "\"\"\")\n",
    "# Positive \n",
    "\n",
    "# review 3\n",
    "review_lst.append(  \"\"\"\n",
    "Great Content. The only problem I faced was with the statistics part. It was rushed and I wasn't able to understand the concept completely and moreover the bigger problem I faced was that there was a huge gap between the theoretical part and the coding part. The big Question to why and when to use a specific theorem was unanswered everytime.\n",
    "Sudhanshu Sir taught amazingly for each and every ML algorithm. The working of every algorithm was explained in a great fashion.\n",
    "\"\"\")\n",
    "# Positive \n",
    "\n",
    "# review 4\n",
    "review_lst.append(\"\"\"\n",
    "Bigdata course is not good. Instructor is well knowledge but not able convey that to in convincing manner . he is in hurry to complete the concept . I am upset with the course\n",
    "\"\"\")\n",
    "# Negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF = (Term Frequency in doc j)/(Total words in doc j)\n",
    "\n",
    "IDF = log2(Total document/No of Documents with Term)\n",
    "\n",
    "TF * IDF -> will use for each unique word column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best\n",
      "coaching\n",
      "ever\n",
      "found\n",
      "online\n",
      "Shudhanshu\n",
      "Krish\n",
      "amazing\n",
      "teaching\n",
      "skill\n",
      "You\n",
      "guys\n",
      "really\n",
      "Great\n",
      "society\n",
      "well\n",
      "providing\n",
      "affordable\n",
      "coarse\n",
      "Thanks\n",
      "Lot\n",
      "I\n",
      "enrolled\n",
      "Machine\n",
      "Learning\n",
      "Deep\n",
      "learning\n",
      "course\n",
      "The\n",
      "course\n",
      "content\n",
      "good\n",
      "way\n",
      "even\n",
      "person\n",
      "Non-IT\n",
      "background\n",
      "understand\n",
      "well\n",
      "Krish\n",
      "Sudanshu\n",
      "You\n",
      "people\n",
      "really\n",
      "awesome\n",
      "I\n",
      "lucky\n",
      "teachers\n",
      "like\n",
      "you.The\n",
      "support\n",
      "team\n",
      "quick\n",
      "highly\n",
      "responsive.The\n",
      "course\n",
      "fee\n",
      "highly\n",
      "affordable.You\n",
      "making\n",
      "career\n",
      "transitions\n",
      "smoother\n",
      "us.I\n",
      "hope\n",
      "able\n",
      "career\n",
      "transition\n",
      "guidance\n",
      "next\n",
      "3-4\n",
      "months\n",
      "Great\n",
      "Content\n",
      "The\n",
      "problem\n",
      "I\n",
      "faced\n",
      "statistics\n",
      "part\n",
      "It\n",
      "rushed\n",
      "I\n",
      "n't\n",
      "able\n",
      "understand\n",
      "concept\n",
      "completely\n",
      "moreover\n",
      "bigger\n",
      "problem\n",
      "I\n",
      "faced\n",
      "huge\n",
      "gap\n",
      "theoretical\n",
      "part\n",
      "coding\n",
      "part\n",
      "The\n",
      "big\n",
      "Question\n",
      "use\n",
      "specific\n",
      "theorem\n",
      "unanswered\n",
      "everytime\n",
      "Sudhanshu\n",
      "Sir\n",
      "taught\n",
      "amazingly\n",
      "every\n",
      "ML\n",
      "algorithm\n",
      "The\n",
      "working\n",
      "every\n",
      "algorithm\n",
      "explained\n",
      "great\n",
      "fashion\n",
      "Bigdata\n",
      "course\n",
      "good\n",
      "Instructor\n",
      "well\n",
      "knowledge\n",
      "able\n",
      "convey\n",
      "convincing\n",
      "manner\n",
      "hurry\n",
      "complete\n",
      "concept\n",
      "I\n",
      "upset\n",
      "course\n",
      "{\"'convey\", '3-4', \"'\", \"'course\", \"'ML\", \"'us.I\", \"'rushed\", \"'Non-IT\", \"'transitions\", \"'learning\", \"'unanswered\", \"'completely\", \"'transition\", ',', \"'guidance\", \"'coaching\", \"'bigger\", \"'next\", \"'Deep\", \"'Sudhanshu\", \"'everytime\", \"'smoother\", \"'Lot\", \"'Instructor\", \"'understand\", \"'fashion\", \"'upset\", \"'knowledge\", '``', \"'You\", \"'affordable.You\", \"'even\", \"'Best\", \"'Great\", \"'statistics\", \"'career\", \"'part\", \"'awesome\", \"'convincing\", \"'The\", \"'huge\", \"'providing\", \"'moreover\", \"'society\", \"'team\", \"'hope\", \"'quick\", \"'great\", \"'theorem\", \"'faced\", \"'big\", \"'skill\", \"'responsive.The\", \"'working\", \"'use\", \"'Krish\", \"'months\", \"'taught\", \"'Question\", \"'content\", \"'Machine\", ']', \"'concept\", '[', \"'Sudanshu\", \"'Bigdata\", \"'fee\", \"'amazing\", \"'guys\", \"'ever\", \"'problem\", \"''\", \"'lucky\", \"'amazingly\", \"'person\", \"'you.The\", \"'people\", \"'found\", \"'Shudhanshu\", \"'theoretical\", \"'It\", \"'Sir\", \"'like\", 'I', \"'affordable\", \"'teachers\", \"'specific\", \"'making\", \"'Content\", \"'good\", \"'gap\", \"'way\", \"'able\", \"'complete\", \"'support\", \"'Learning\", \"'coarse\", \"'online\", \"'background\", \"'highly\", \"'teaching\", \"'algorithm\", \"'Thanks\", \"'well\", \"'every\", \"'hurry\", \"'explained\", \"'manner\", \"'really\", \"'coding\", \"'enrolled\", \"n't\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "refined = []\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "for review in review_lst:\n",
    "    lst = []\n",
    "    for word in nltk.word_tokenize(review):\n",
    "        if word.strip() not in punctuation and word.strip() not in [']','['] and word.strip() not in [str(x) for x in list(range(1,10))]:\n",
    "            if word not in stop_words:\n",
    "                print(word)\n",
    "                lst.append(word)\n",
    "                \n",
    "    refined.append(str(lst))\n",
    "\n",
    "#print(refined)\n",
    "merged = ''\n",
    "for i in refined:\n",
    "    merged= merged + i\n",
    "    \n",
    "#print(merged)    \n",
    "cols = set(nltk.word_tokenize(merged))\n",
    "\n",
    "print(cols)\n",
    "df = pd.DataFrame(columns=cols)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'convey</th>\n",
       "      <th>3-4</th>\n",
       "      <th>'</th>\n",
       "      <th>'course</th>\n",
       "      <th>'ML</th>\n",
       "      <th>'us.I</th>\n",
       "      <th>'rushed</th>\n",
       "      <th>'Non-IT</th>\n",
       "      <th>'transitions</th>\n",
       "      <th>'learning</th>\n",
       "      <th>'unanswered</th>\n",
       "      <th>'completely</th>\n",
       "      <th>'transition</th>\n",
       "      <th>,</th>\n",
       "      <th>'guidance</th>\n",
       "      <th>'coaching</th>\n",
       "      <th>'bigger</th>\n",
       "      <th>'next</th>\n",
       "      <th>'Deep</th>\n",
       "      <th>'Sudhanshu</th>\n",
       "      <th>'everytime</th>\n",
       "      <th>'smoother</th>\n",
       "      <th>'Lot</th>\n",
       "      <th>'Instructor</th>\n",
       "      <th>'understand</th>\n",
       "      <th>'fashion</th>\n",
       "      <th>'upset</th>\n",
       "      <th>'knowledge</th>\n",
       "      <th>``</th>\n",
       "      <th>'You</th>\n",
       "      <th>'affordable.You</th>\n",
       "      <th>'even</th>\n",
       "      <th>'Best</th>\n",
       "      <th>'Great</th>\n",
       "      <th>'statistics</th>\n",
       "      <th>'career</th>\n",
       "      <th>'part</th>\n",
       "      <th>'awesome</th>\n",
       "      <th>'convincing</th>\n",
       "      <th>'The</th>\n",
       "      <th>'huge</th>\n",
       "      <th>'providing</th>\n",
       "      <th>'moreover</th>\n",
       "      <th>'society</th>\n",
       "      <th>'team</th>\n",
       "      <th>'hope</th>\n",
       "      <th>'quick</th>\n",
       "      <th>'great</th>\n",
       "      <th>'theorem</th>\n",
       "      <th>'faced</th>\n",
       "      <th>'big</th>\n",
       "      <th>'skill</th>\n",
       "      <th>'responsive.The</th>\n",
       "      <th>'working</th>\n",
       "      <th>'use</th>\n",
       "      <th>'Krish</th>\n",
       "      <th>'months</th>\n",
       "      <th>'taught</th>\n",
       "      <th>'Question</th>\n",
       "      <th>'content</th>\n",
       "      <th>'Machine</th>\n",
       "      <th>]</th>\n",
       "      <th>'concept</th>\n",
       "      <th>[</th>\n",
       "      <th>'Sudanshu</th>\n",
       "      <th>'Bigdata</th>\n",
       "      <th>'fee</th>\n",
       "      <th>'amazing</th>\n",
       "      <th>'guys</th>\n",
       "      <th>'ever</th>\n",
       "      <th>'problem</th>\n",
       "      <th>''</th>\n",
       "      <th>'lucky</th>\n",
       "      <th>'amazingly</th>\n",
       "      <th>'person</th>\n",
       "      <th>'you.The</th>\n",
       "      <th>'people</th>\n",
       "      <th>'found</th>\n",
       "      <th>'Shudhanshu</th>\n",
       "      <th>'theoretical</th>\n",
       "      <th>'It</th>\n",
       "      <th>'Sir</th>\n",
       "      <th>'like</th>\n",
       "      <th>I</th>\n",
       "      <th>'affordable</th>\n",
       "      <th>'teachers</th>\n",
       "      <th>'specific</th>\n",
       "      <th>'making</th>\n",
       "      <th>'Content</th>\n",
       "      <th>'good</th>\n",
       "      <th>'gap</th>\n",
       "      <th>'way</th>\n",
       "      <th>'able</th>\n",
       "      <th>'complete</th>\n",
       "      <th>'support</th>\n",
       "      <th>'Learning</th>\n",
       "      <th>'coarse</th>\n",
       "      <th>'online</th>\n",
       "      <th>'background</th>\n",
       "      <th>'highly</th>\n",
       "      <th>'teaching</th>\n",
       "      <th>'algorithm</th>\n",
       "      <th>'Thanks</th>\n",
       "      <th>'well</th>\n",
       "      <th>'every</th>\n",
       "      <th>'hurry</th>\n",
       "      <th>'explained</th>\n",
       "      <th>'manner</th>\n",
       "      <th>'really</th>\n",
       "      <th>'coding</th>\n",
       "      <th>'enrolled</th>\n",
       "      <th>n't</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: ['convey, 3-4, ', 'course, 'ML, 'us.I, 'rushed, 'Non-IT, 'transitions, 'learning, 'unanswered, 'completely, 'transition, ,, 'guidance, 'coaching, 'bigger, 'next, 'Deep, 'Sudhanshu, 'everytime, 'smoother, 'Lot, 'Instructor, 'understand, 'fashion, 'upset, 'knowledge, ``, 'You, 'affordable.You, 'even, 'Best, 'Great, 'statistics, 'career, 'part, 'awesome, 'convincing, 'The, 'huge, 'providing, 'moreover, 'society, 'team, 'hope, 'quick, 'great, 'theorem, 'faced, 'big, 'skill, 'responsive.The, 'working, 'use, 'Krish, 'months, 'taught, 'Question, 'content, 'Machine, ], 'concept, [, 'Sudanshu, 'Bigdata, 'fee, 'amazing, 'guys, 'ever, 'problem, '', 'lucky, 'amazingly, 'person, 'you.The, 'people, 'found, 'Shudhanshu, 'theoretical, 'It, 'Sir, 'like, I, 'affordable, 'teachers, 'specific, 'making, 'Content, 'good, 'gap, 'way, 'able, 'complete, 'support, 'Learning, 'coarse, 'online, 'background, 'highly, ...]\n",
       "Index: []"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
